{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import rdkit as rd\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from toxicity.model import MTDNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter('tensorboard/')\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.enabled=False\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "morgan_bits = 4096\n",
    "morgan_radius = 2\n",
    "train_epoch = 50\n",
    "batch = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "clintox_task = ['CT_TOX']\n",
    "tox21_tasks = ['NR-AR', 'NR-Aromatase', 'NR-PPAR-gamma', 'SR-HSE', \n",
    "               'NR-AR-LBD', 'NR-ER', 'SR-ARE', 'SR-MMP',\n",
    "               'NR-AhR', 'NR-ER-LBD', 'SR-ATAD5', 'SR-p53']\n",
    "\n",
    "all_tasks = tox21_tasks + clintox_task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/toxicity/datasets/tox21/split_data/seed_124/\"\n",
    "train_data=torch.load(data_path + 'train_data_tox21.pth')\n",
    "test_data=torch.load(data_path + 'test_data_tox21.pth')\n",
    "valid_data=torch.load(data_path + 'valid_data_tox21.pth')\n",
    "\n",
    "clintox_data_path = \"./data/toxicity/datasets/clintox/split_data/seed_124/\"\n",
    "train_data_clintox=torch.load(clintox_data_path + 'train_data_clintox.pth')\n",
    "test_data_clintox=torch.load(clintox_data_path + 'test_data_clintox.pth')\n",
    "valid_data_clintox=torch.load(clintox_data_path + 'valid_data_clintox.pth')\n",
    "\n",
    "train_data = train_data.merge(train_data_clintox, how='outer', on='smiles')\n",
    "test_data  = test_data.merge(test_data_clintox, how='outer', on='smiles')\n",
    "valid_data  = valid_data.merge(valid_data_clintox, how='outer', on='smiles')\n",
    "\n",
    "\n",
    "data = [train_data, test_data, valid_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_embed = torch.load(\"./data/toxicity/smiles_embedding/smiles_embed_pretrain.pt\")\n",
    "for i in range(len(data)):\n",
    "    data[i]['smiles_embed'] = data[i]['smiles'].apply(lambda x: smiles_embed.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0] = data[0].fillna(-1)\n",
    "data[1] = data[1].fillna(-1)\n",
    "data[2] = data[2].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[0]\n",
    "test_data  = data[1]\n",
    "valid_data = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for tensor in train_data['smiles_embed']:\n",
    "    x_train.append(tensor)\n",
    "\n",
    "x_train = torch.stack(x_train)\n",
    "x_train = x_train.numpy()\n",
    "\n",
    "y_train = train_data[all_tasks].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "\n",
    "for tensor in test_data['smiles_embed']:\n",
    "    x_test.append(tensor)\n",
    "x_test = torch.stack(x_test)\n",
    "x_test = x_test.numpy()\n",
    "\n",
    "y_test = test_data[all_tasks].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = []\n",
    "for tensor in valid_data['smiles_embed']:\n",
    "    x_valid.append(tensor)\n",
    "x_valid = torch.stack(x_valid)\n",
    "x_valid = x_valid.numpy()\n",
    "\n",
    "    \n",
    "y_valid = valid_data[all_tasks].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = np.sum(y_train >= 0, 0)\n",
    "N_test  = np.sum(y_test >= 0, 0)\n",
    "N_valid  = np.sum(y_valid >= 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_torch = x_train.astype(np.float32)\n",
    "y_train_torch = y_train.astype(np.float32)\n",
    "\n",
    "x_test_torch = x_test.astype(np.float32)\n",
    "y_test_torch = y_test.astype(np.float32)\n",
    "\n",
    "x_valid_torch = x_valid.astype(np.float32)\n",
    "y_valid_torch = y_valid.astype(np.float32)\n",
    "\n",
    "input_shape = x_train_torch.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTDNNData(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = MTDNNData(x_train_torch, y_train_torch)\n",
    "training_generator = DataLoader(training_set, batch_size=batch, shuffle=True)\n",
    "\n",
    "testing_set = MTDNNData(x_test_torch, y_test_torch)\n",
    "testing_generator = DataLoader(testing_set, batch_size=len(testing_set), shuffle=False)\n",
    "\n",
    "valid_set = MTDNNData(x_valid_torch, y_valid_torch)\n",
    "valid_generator = DataLoader(valid_set, batch_size=len(valid_set), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    # Method from : https://gist.github.com/vsay01/45dfced69687077be53dbdd4987b6b17\n",
    "    f_path = checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "        \n",
    "def load_ckp(checkpoint_fpath, input_model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    input_model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    train_loss_min = checkpoint['train_loss_min']\n",
    "    return model, optimizer, checkpoint['epoch'], train_loss_min.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"./checkpoints/toxicity\"\n",
    "\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.mkdir(ckpt_path)\n",
    "\n",
    "checkpoint_file= ckpt_path + '/current_checkpoint.pt'\n",
    "bestmodel_file = ckpt_path + '/best_model.pt'  \n",
    "bestmodel_byvalid_file = ckpt_path + '/best_model_by_valid.pt' \n",
    "bestmodel_byvalid_crossed_file = ckpt_path + '/best_model_by_valid-crossed.pt'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = MTDNN(input_shape, all_tasks).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47685/2214527620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mrunning_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trial/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trial/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trial/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    250\u001b[0m                  \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fused'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                  \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                  found_inf=found_inf)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trial/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m          \u001b[0mdifferentiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m          \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m          found_inf=found_inf)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trial/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mstep_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history=[]  \n",
    "correct_history=[]  \n",
    "val_loss_history=[]  \n",
    "val_correct_history=[] \n",
    "train_loss_min = np.Inf\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "for e in range(train_epoch):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    running_train_loss = 0\n",
    "    running_valid_loss = 0\n",
    "    running_train_correct = 0\n",
    "    running_val_correct = 0\n",
    "    y_train_true = []\n",
    "    y_train_pred = []\n",
    "    y_valid_true = []\n",
    "    y_valid_pred = []\n",
    "    batch = 0\n",
    "    for x_batch, y_batch in training_generator:\n",
    "        batch += 1\n",
    "        if torch.cuda.is_available():\n",
    "            x_batch, y_batch = x_batch.cuda(), y_batch.cuda() \n",
    "        \n",
    "        y_pred = model(x_batch)\n",
    "        \n",
    "        # Compute loss over all tasks\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        y_train_true_task = []\n",
    "        y_train_pred_task = []\n",
    "        for i in range(len(all_tasks)):\n",
    "            y_batch_task = y_batch[:,i]\n",
    "            y_pred_task  = y_pred[i][:,0]\n",
    "            \n",
    "            indice_valid = y_batch_task >= 0\n",
    "            loss_task = criterion(y_pred_task[indice_valid], y_batch_task[indice_valid]) / N_train[i]\n",
    "            \n",
    "            loss += loss_task\n",
    "\n",
    "            pred_train = np.round(y_pred_task[indice_valid].detach().cpu().numpy())\n",
    "            target_train = y_batch_task[indice_valid].float()\n",
    "            y_train_true.extend(target_train.tolist()) \n",
    "            y_train_pred.extend(pred_train.reshape(-1).tolist())\n",
    "\n",
    "        writer.add_scalar(\"Accuracy/train\", loss, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_train_loss += loss.item()\n",
    "        writer.add_scalar(\"Loss/train\", running_train_loss, e)\n",
    "        \n",
    "    else:\n",
    "        with torch.no_grad():    \n",
    "        ## evaluation part \n",
    "            model.eval()\n",
    "            for val_x_batch, val_y_batch in valid_generator:\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    val_x_batch, val_y_batch = val_x_batch.cuda(), val_y_batch.cuda() \n",
    "                \n",
    "                val_output = model(val_x_batch)\n",
    "\n",
    "                ## 2. loss calculation over all tasks \n",
    "                val_loss = 0\n",
    "                val_correct = 0\n",
    "                y_valid_true_task = []\n",
    "                y_valid_pred_task = []\n",
    "                for i in range(len(all_tasks)):\n",
    "                    val_y_batch_task = val_y_batch[:,i]\n",
    "                    val_output_task  = val_output[i][:,0]\n",
    "\n",
    "                    # compute loss for labels that are not NA\n",
    "                    indice_valid = val_y_batch_task >= 0\n",
    "                    val_loss_task = criterion(val_output_task[indice_valid], val_y_batch_task[indice_valid]) / N_valid[i]\n",
    "\n",
    "                    val_loss += val_loss_task\n",
    "                    \n",
    "                    pred_valid = np.round(val_output_task[indice_valid].detach().cpu().numpy())\n",
    "                    target_valid = val_y_batch_task[indice_valid].float()\n",
    "                    y_valid_true.extend(target_valid.tolist()) \n",
    "                    y_valid_pred.extend(pred_valid.reshape(-1).tolist())\n",
    "                \n",
    "\n",
    "                running_valid_loss+=val_loss.item()\n",
    "                writer.add_scalar(\"Loss/valid\", running_valid_loss, e)\n",
    "        \n",
    "        #epoch loss\n",
    "        train_epoch_loss=np.mean(running_train_loss)\n",
    "        val_epoch_loss=np.mean(running_valid_loss)  \n",
    "       \n",
    "        #epoch accuracy     \n",
    "        train_epoch_acc = accuracy_score(y_train_true,y_train_pred)\n",
    "        val_epoch_acc = accuracy_score(y_valid_true,y_valid_pred)\n",
    "        \n",
    "        #history\n",
    "        loss_history.append(train_epoch_loss)  \n",
    "        correct_history.append(train_epoch_acc)\n",
    "        val_loss_history.append(val_epoch_loss)  \n",
    "        val_correct_history.append(val_epoch_acc)  \n",
    "        \n",
    "        print(f\"Epoch: {e}, Training loss: {train_epoch_loss:.4f}, Valid loss: {val_epoch_loss:.4f}\")\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': e + 1,\n",
    "            'train_loss_min': train_epoch_loss,\n",
    "            'val_loss_min': val_epoch_loss, \n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "        save_ckp(checkpoint, False, checkpoint_file, bestmodel_file)\n",
    "        \n",
    "        if train_epoch_loss <= train_loss_min:\n",
    "            save_ckp(checkpoint, True, checkpoint_file, bestmodel_file)\n",
    "            train_loss_min = train_epoch_loss\n",
    "            \n",
    "        if train_epoch_loss >= val_epoch_loss:\n",
    "            save_ckp(checkpoint, True, checkpoint_file, bestmodel_byvalid_crossed_file)\n",
    "            train_loss_min = train_epoch_loss\n",
    "            \n",
    "        if val_epoch_loss <= val_loss_min:\n",
    "            save_ckp(checkpoint, True, checkpoint_file, bestmodel_file)\n",
    "            val_loss_min = val_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model, optimizer, start_epoch, train_loss_min = load_ckp(bestmodel_file, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task       \t   AUC   ACC   BACC   TN    TP    PR    RC    F1  \n",
      "NR-AR      \t [0.552 0.469 0.516 0.464 0.567 0.514 0.567 0.079]\n",
      "NR-Aromatase \t [0.569 0.352 0.546 0.327 0.765 0.532 0.765 0.119]\n",
      "NR-PPAR-gamma \t [0.491 0.559 0.53  0.56  0.5   0.532 0.5   0.045]\n",
      "SR-HSE     \t [0.575 0.698 0.526 0.718 0.333 0.542 0.333 0.105]\n",
      "NR-AR-LBD  \t [0.42  0.522 0.411 0.528 0.294 0.384 0.294 0.029]\n",
      "NR-ER      \t [0.537 0.603 0.537 0.631 0.443 0.546 0.443 0.249]\n",
      "SR-ARE     \t [0.472 0.434 0.469 0.417 0.521 0.472 0.521 0.231]\n",
      "SR-MMP     \t [0.472 0.56  0.453 0.611 0.295 0.431 0.295 0.177]\n",
      "NR-AhR     \t [0.453 0.583 0.493 0.611 0.375 0.491 0.375 0.175]\n",
      "NR-ER-LBD  \t [0.543 0.289 0.526 0.256 0.795 0.517 0.795 0.12 ]\n",
      "SR-ATAD5   \t [0.587 0.48  0.57  0.473 0.667 0.559 0.667 0.077]\n",
      "SR-p53     \t [0.51  0.492 0.511 0.489 0.533 0.511 0.533 0.12 ]\n",
      "CT_TOX     \t [0.268 0.304 0.22  0.314 0.125 0.154 0.125 0.019]\n"
     ]
    }
   ],
   "source": [
    "print('Task'.ljust(10), '\\t', '  AUC ', ' ACC ', ' BACC ', ' TN  ', ' TP  ', ' PR  ', ' RC  ', ' F1  ')\n",
    "for task, auc in results.items():\n",
    "    print(task.ljust(10), '\\t', np.round(auc,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print test loss\n",
    "for x_valid_torch, y_valid_torch in valid_generator:\n",
    "    y_valid_pred = model.eval().to(device).cpu()(x_valid_torch)\n",
    "    \n",
    "    # Compute loss over all tasks\n",
    "    loss = 0\n",
    "    for i in range(len(all_tasks)):\n",
    "        y_test_task = y_valid_torch[:,i]\n",
    "        y_pred_task  = y_valid_pred[i][:,0]\n",
    "\n",
    "        # compute loss for labels that are not NA\n",
    "        indice_valid = y_test_task >= 0\n",
    "        loss_task = criterion(y_pred_task[indice_valid], y_test_task[indice_valid]) / N_test[i]\n",
    "\n",
    "        loss += loss_task\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_valid = {}\n",
    "# Collects performance metrics for all tasks on Valid set\n",
    "for i in range(len(all_tasks)):\n",
    "    \n",
    "    valid_datapoints = y_valid[:,i] >= 0\n",
    "    y_valid_task = y_valid[valid_datapoints,i] \n",
    "    y_valid_pred_task = y_valid_pred[i].detach().numpy()[valid_datapoints,0]\n",
    "    \n",
    "    \n",
    "    acc = accuracy_score(y_valid_task, np.round(y_valid_pred_task))\n",
    "    print('Accuracy for deepnn on Morgan Fingerprint:', acc)\n",
    "    \n",
    "    bacc = sk.metrics.balanced_accuracy_score(y_valid_task, np.round(y_valid_pred_task))\n",
    "\n",
    "    f1 = f1_score(y_valid_task, np.round(y_valid_pred_task), pos_label=1)\n",
    "    print('F1 for deepnn on Morgan Fingerprint:', f1)\n",
    "\n",
    "    cfm = sk.metrics.confusion_matrix(y_valid_task, np.round(y_valid_pred_task))\n",
    "    cfm = cfm.astype('float') / cfm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print('Confusion Matrix for deepnn on Morgan Fingerprint:\\n', cfm)\n",
    "\n",
    "    tn, fp, fn, tp = cfm.ravel()\n",
    "    pr = tp / (tp + fp)\n",
    "    rc = tp / (tp + fn)\n",
    "    print(' True Positive:', tp)\n",
    "    print(' True Negative:', tn)\n",
    "    print('False Positive:', fp)\n",
    "    print('False Negative:', fn)\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(y_valid_task, y_valid_pred_task)\n",
    "    print('Test ROC AUC ({}):'.format(all_tasks[i]), auc)\n",
    "    \n",
    "    results_valid[all_tasks[i]] = [auc, acc, bacc, tn, tp, pr, rc, f1]\n",
    "\n",
    "    fpr, tpr, threshold = sk.metrics.roc_curve(y_valid_task, y_valid_pred_task)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Task'.ljust(10), '\\t', '  AUC ', ' ACC ', ' BACC ', ' TN  ', ' TP  ', ' PR  ', ' RC  ', ' F1  ')\n",
    "for task, auc in results_valid.items():\n",
    "    print(task.ljust(10), '\\t', np.round(auc,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
